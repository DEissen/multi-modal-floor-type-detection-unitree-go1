{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# custom imports\n",
    "from FTDDataset.FTDDataset import FloorTypeDetectionDataset\n",
    "from models.unimodal_models import LeNet_Like, VGG_Like\n",
    "from models.multimodal_models import LeNet_Like_multimodal\n",
    "from train import Trainer\n",
    "from eval import evaluate, load_state_dict\n",
    "from visualization.visualization import visualize_data_sample_or_batch\n",
    "from custom_utils.custom_utils import gen_run_dir, start_logger, store_used_config\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for dataset and config to use\n",
    "dataset_path = r\"C:\\Users\\Dominik\\Downloads\\FTDD_1.0\"\n",
    "mapping_filename = \"label_mapping_full_dataset.json\"\n",
    "preprocessing_config_filename = \"preprocessing_config.json\"\n",
    "\n",
    "# list of sensors to use\n",
    "# sensors = ['accelerometer', 'BellyCamLeft', 'BellyCamRight', 'bodyHeight', 'ChinCamLeft', 'ChinCamRight', 'footForce', 'gyroscope',\n",
    "#            'HeadCamLeft', 'HeadCamRight', 'LeftCamLeft', 'LeftCamRight', 'mode', 'RightCamLeft', 'RightCamRight', 'rpy', 'velocity', 'yawSpeed']\n",
    "sensors = [\"BellyCamLeft\", \"HeadCamLeft\", \"accelerometer\", \"footForce\", \"gyroscope\", \"LeftCamRight\", \"RightCamRight\"]\n",
    "\n",
    "# config for training\n",
    "train_config_dict = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 8,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"num_classes\": 4,\n",
    "    \"use_wandb\": True,\n",
    "    \"visualize_results\": True,\n",
    "    \"train_log_interval\": 200,\n",
    "    \"sensors\": sensors,\n",
    "    \"dataset_path\": dataset_path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_paths_dict = gen_run_dir(r\"D:\\git_repos\\FA_DL_Modelle\\runs\\run_31_08__20_46_54\")\n",
    "start_logger(run_paths_dict[\"logs_path\"], stream_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "transformed_dataset = FloorTypeDetectionDataset(\n",
    "    dataset_path, sensors, mapping_filename, preprocessing_config_filename)\n",
    "\n",
    "# get all possible config dicts for logging\n",
    "label_mapping_dict = transformed_dataset.get_mapping_dict()\n",
    "preprocessing_config_dict = transformed_dataset.get_preprocessing_config()\n",
    "\n",
    "# split in train and test dataset\n",
    "train_size = int(0.8 * len(transformed_dataset))\n",
    "test_size = len(transformed_dataset) - train_size\n",
    "ds_train, ds_test = torch.utils.data.random_split(\n",
    "    transformed_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, loss and optimizer\n",
    "# model = LeNet_Like(train_config_dict[\"num_classes\"])\n",
    "model = VGG_Like(train_config_dict[\"num_classes\"], train_config_dict[\"dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## multimodal models\n",
    "# determine number of input features for all timeseries sensors\n",
    "num_input_features_dict = {}\n",
    "for sensor in sensors:\n",
    "    if not \"Cam\" in sensor:\n",
    "        _, (training_sample, _) = next(enumerate(transformed_dataset))\n",
    "        num_input_features_dict[sensor] = training_sample[sensor].size()[0]\n",
    "# define multimodal model\n",
    "model = LeNet_Like_multimodal(train_config_dict[\"num_classes\"], sensors, num_input_features_dict, train_config_dict[\"dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "trainer = Trainer(model, ds_train, ds_test, sensors,\n",
    "                    train_config_dict, run_paths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally load instead of train the model\n",
    "num = 2\n",
    "load_path = os.path.join(run_paths_dict[\"model_ckpts\"], f\"{model._get_name()}_{num}.pt\")\n",
    "load_state_dict(model, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract state dict\n",
    "params_dict = model.state_dict()\n",
    "\n",
    "weights = params_dict[\"classification_layers.0.weight\"].numpy()\n",
    "biases = params_dict[\"classification_layers.0.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed infos about feature shape\n",
    "flatten_length_cam_feature = 16 * 13 ** 2\n",
    "flatten_length_IMU_feature = 16 * 9\n",
    "\n",
    "# normalize weights\n",
    "min_val = np.min(weights)\n",
    "norm_weights = weights - min_val\n",
    "max_val = np.max(norm_weights)\n",
    "norm_weights = norm_weights / max_val\n",
    "\n",
    "# extract weights for layers\n",
    "weights = norm_weights\n",
    "extracted_weights = []\n",
    "current_pos = 0\n",
    "for sensor in sensors:\n",
    "    if \"Cam\" in sensor:\n",
    "        new_pos = (current_pos+flatten_length_cam_feature)\n",
    "        extracted_weights.append(weights[:, current_pos:new_pos])\n",
    "        current_pos = new_pos\n",
    "    else:\n",
    "        new_pos = (current_pos+flatten_length_IMU_feature)\n",
    "        extracted_weights.append(weights[:, current_pos:new_pos])\n",
    "        current_pos = new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "rows = 3\n",
    "columns =3\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = []\n",
    "\n",
    "# add all subplots\n",
    "for index, cur_weight in enumerate(extracted_weights):\n",
    "    # add next subplot and title\n",
    "    ax.append(fig.add_subplot(rows, columns, index+1))\n",
    "    ax[-1].set_title(sensors[index])\n",
    "    plt.imshow(cur_weight.transpose(1,0), cmap=\"viridis\", vmin=0, vmax = 1, interpolation=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete plot (to big)\n",
    "plt.imshow(test[\"classification_layers.3.weight\"].numpy(), cmap=\"viridis\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, ds_train, sensors, train_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store used config as final step of logging\n",
    "store_used_config(run_paths_dict, label_mapping_dict, preprocessing_config_dict, train_config_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
